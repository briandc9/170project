{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from student_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import student_utils as s_utils\n",
    "import utils\n",
    "import dwave_networkx as d_nx\n",
    "import dimod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================================================  \n",
    "# Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================================\n",
    "  Complete the following function.\n",
    "======================================================================\n",
    "\"\"\"\n",
    "\n",
    "def solve(list_of_locations, list_of_homes, starting_car_location, adjacency_matrix, params=[]):\n",
    "    \"\"\"\n",
    "    Write your algorithm here.\n",
    "    Input:\n",
    "        list_of_locations: A list of locations such that node i of the graph corresponds to name at index i of the list\n",
    "        list_of_homes: A list of homes\n",
    "        starting_car_location: The name of the starting location for the car\n",
    "        adjacency_matrix: The adjacency matrix from the input file\n",
    "    Output:\n",
    "        A list of locations representing the car path\n",
    "        A dictionary mapping drop-off location to a list of homes of TAs that got off at that particular location\n",
    "        NOTE: both outputs should be in terms of indices not the names of the locations themselves\n",
    "    \"\"\"\n",
    "    # Make networkx graph G\n",
    "    #G = make_graph(adjacency_matrix)\n",
    "    G, message = s_utils.adjacency_matrix_to_graph(adjacency_matrix)\n",
    "    #print(\"G:\", G.edges())\n",
    "    \n",
    "    # Get indices of homes\n",
    "    home_indices = [list_of_locations.index(i) for i in list_of_homes]\n",
    "    location_indices = range(0, len(list_of_locations))\n",
    "    \n",
    "    starting_index = list_of_locations.index(starting_car_location)\n",
    "    #print(\"start index:\", starting_index)\n",
    "    \n",
    "    # Get adjacency dictionary of distances for each location\n",
    "    adjacencies = make_dictionary(adjacency_matrix, location_indices)\n",
    "    #print(\"adjacencies:\", adjacencies)\n",
    "    \n",
    "    # Get nodes\n",
    "    nodes = make_nodes(adjacencies, location_indices, home_indices, starting_index)\n",
    "    node_roots = list(nodes.keys())\n",
    "    #print(\"node_roots:\", node_roots)\n",
    "    \n",
    "    # Create graph of just nodes\n",
    "    node_paths, node_G = shortest_paths(G, list_of_locations, node_roots)\n",
    "    #print(\"node_G edges:\", node_G.edges())\n",
    "    #print(\"edge (5,6)\", node_G.get_edge_data(5, 6,default=0) )\n",
    "    #print(\"edge (5,8)\", node_G.get_edge_data(5, 8,default=0) )\n",
    "    #print(node_G.nodes())\n",
    "        \n",
    "    # TSP on node_G\n",
    "    #path = tsp_solver(node_G, starting_index)\n",
    "    tsp_path = christofedes(node_G, starting_index)\n",
    "    #print(\"node_path:\", tsp_path)\n",
    "    \n",
    "    # Output path for driver\n",
    "    output_path = make_path(list_of_locations, node_paths, tsp_path, starting_index)\n",
    "    #print(\"output path:\", output_path)\n",
    "    \n",
    "    # Drop off points and TAs dropped\n",
    "    #dropoff_mapping = dicitonary of {dropoff_loc: [list of TAs dropped off], ...} \n",
    "    dropoff_mapping = dropoffs(output_path, nodes, tsp_path, home_indices, list_of_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert adjacency matrix -> pd DataFrame -> networkx Graph\n",
    "def make_graph(adjacency_matrix):\n",
    "    adj = adjacency_matrix\n",
    "    \n",
    "    df = pd.DataFrame(adj)\n",
    "    df = df.replace(\"x\", 0)\n",
    "        \n",
    "    G = nx.from_pandas_adjacency(df)\n",
    "    G.name = 'Graph from pandas adjacency matrix'\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns adjacency dictionary of distances for each location {location: [distances to every location], ...}\n",
    "\n",
    "def make_dictionary(adjacency_matrix, location_indices):\n",
    "    #Create dictionary for every location\n",
    "    adjacencies = {}\n",
    "    \n",
    "    #Create dictionary of adjacencent locations for every location\n",
    "    # adjacencies = {\"loc\" : [distance to every other loc], ...}\n",
    "    # If distance == \"x\" -> None\n",
    "    for i in location_indices:\n",
    "        adj = [None if j == \"x\" else j for j in adjacency_matrix[i]]\n",
    "        adjacencies[i] = adj\n",
    "    \n",
    "    return adjacencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TODO: Change nodes to locations instead of just homes\n",
    "Guarantee 'start' location is a node\"\"\" \n",
    "# Returns dictionary of {node_home: [homes belonging to node], ...}\n",
    "\n",
    "#locations = list of indices\n",
    "\n",
    "def make_nodes(adjacencies, locations, home_indices, starting_index):    \n",
    "    limit = 12000 #maximum distance away from node's base\n",
    "    nodes = {} #create a node around every location\n",
    "    \n",
    "    #Create nodes for every home\n",
    "    for loc in locations:\n",
    "        if loc in home_indices:\n",
    "            nodes[loc] = list()\n",
    "            nodes[loc].append(loc) #Start every home's node with itself\n",
    "        else:\n",
    "            nodes[loc] = list()\n",
    "        \n",
    "        for index in home_indices:\n",
    "            distance = adjacencies[loc][index]\n",
    "            if (distance != None) and (distance < limit):\n",
    "                #append other home that is within limit to the node starting at that home\n",
    "                #current = nodes[loc]\n",
    "                nodes[loc].append(index) #returns None    \n",
    "    \n",
    "    #Clean up node dictionary to only contain largest nodes ------------\n",
    "    deleted_nodes = nodes.copy()\n",
    "    homes_represented = home_indices\n",
    "    nodes_to_keep = list()\n",
    "    \n",
    "    #for node in node.keys():\n",
    "    \n",
    "    while homes_represented:        \n",
    "        v = list(deleted_nodes.values())\n",
    "        k = list(deleted_nodes.keys())\n",
    "        biggest_node = k[v.index(max(v, key=len))]\n",
    "        \n",
    "        #remove homes that are already included in list\n",
    "        #print(len(homes_represented))\n",
    "        homes_represented = [x for x in homes_represented if x not in nodes[biggest_node]]\n",
    "        \n",
    "        deleted_nodes.pop(biggest_node, None)\n",
    "        \n",
    "        for home in nodes[biggest_node]:\n",
    "            deleted_nodes.pop(home, None)\n",
    "            #print(\"deleted_nodes:\", deleted_nodes)\n",
    "        \n",
    "        nodes_to_keep.append(biggest_node)\n",
    "    \n",
    "    #print(nodes_to_keep)\n",
    "    \n",
    "    if starting_index not in nodes_to_keep:\n",
    "        nodes_to_keep.append(starting_index)\n",
    "    \n",
    "    return {key: nodes[key] for key in nodes_to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary of shortest paths between nodes {(node_1, node_2): [list of path], ...} \n",
    "#    for outputting driver path\n",
    "#returns new graph of just nodes and their associated distances\n",
    "#    for TSP solving with dwave\n",
    "\n",
    "def shortest_paths(G, list_of_locations, node_roots):\n",
    "    node_paths = {}\n",
    "    #node_distances = {}\n",
    "    \n",
    "    #Make new graph of nodes\n",
    "    node_G = nx.Graph()\n",
    "    \n",
    "    for node in node_roots:\n",
    "        node_G.add_node(node)\n",
    "    \n",
    "    #Get shortest path between every node and the distance of that path\n",
    "    for node_s in node_roots:\n",
    "        node_paths[node_s] = list()\n",
    "        for node_t in node_roots:\n",
    "            #index_s = list_of_locations.index(node_s)\n",
    "            #index_t = list_of_locations.index(node_t)\n",
    "            if (node_s != node_t):\n",
    "                path = nx.shortest_path(G, source = node_s, target = node_t, weight = \"weight\")\n",
    "                node_paths[(node_s, node_t)] = path\n",
    "\n",
    "                path_weight = nx.shortest_path_length(G, source= node_s, target= node_t, weight= \"weight\")\n",
    "                node_G.add_edge(node_s, node_t, weight= path_weight)\n",
    "        \n",
    "    return node_paths, node_G\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSP Solver\n",
    "\n",
    "G (NetworkX graph) – The graph on which to find a minimum traveling salesperson route. This should be a complete graph with non-zero weights on every edge.\n",
    "\n",
    "sampler – A binary quadratic model sampler. A sampler is a process that samples from low energy states in models defined by an Ising equation or a Quadratic Unconstrained Binary Optimization Problem (QUBO). A sampler is expected to have a ‘sample_qubo’ and ‘sample_ising’ method. A sampler is expected to return an iterable of samples, in order of increasing energy. If no sampler is provided, one must be provided using the set_default_sampler function.\n",
    "\n",
    "lagrange (number, optional (default 2)) – Lagrange parameter to weight constraints (visit every city once) versus objective (shortest distance route).\n",
    "\n",
    "weight (optional (default 'weight')) – The name of the edge attribute containing the weight.\n",
    "\n",
    "start (node, optional) – If provided, the route will begin at start.\n",
    "\n",
    "sampler_args – Additional keyword parameters are passed to the sampler.\n",
    "\n",
    "### OR...\n",
    "Use Christofedes algo\n",
    "https://medium.com/musoc17-visualization-of-popular-algorithms/travelling-salesman-problem-e3b98653b11a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dwave tsp.python\n",
    "\n",
    "def tsp_solver(node_G, starting_index):\n",
    "    \n",
    "    return d_nx.traveling_salesperson(node_G, sampler = sample_qubo(), weight='weight', start=starting_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def christofedes(G, starting_node):\n",
    "    #opt_G = nx.DiGraph()\n",
    "    optimal_path = list()\n",
    "    #optimal_dist = 0\n",
    "    \n",
    "    MST = nx.minimum_spanning_tree(G, weight='weight') # generates minimum spanning tree of graph G, using Prim's algo\n",
    "    odd_vert = [] #list containing vertices with odd degree\n",
    "    \n",
    "    for i in MST.nodes():\n",
    "        if MST.degree(i)%2 != 0: \n",
    "            odd_vert.append(i) #if the degree of the vertex is odd, then append it to odd_vert list\n",
    "            \n",
    "    minimumWeightedMatching(MST, G, odd_vert) #adds minimum weight matching edges to MST\n",
    "    \n",
    "    # now MST has the Eulerian circuit\n",
    "    #start = MST.nodes()[0]\n",
    "    start = starting_node\n",
    "    visited = {node: False for node in MST.nodes()}\n",
    "    \n",
    "    # finds the hamiltonian circuit (skips repeated vertices)\n",
    "    curr = start\n",
    "    visited[curr] = True\n",
    "    optimal_path.append(curr)\n",
    "    \n",
    "    for nd in MST.neighbors(curr):\n",
    "        if visited[nd] == False or nd == start:\n",
    "            next = nd\n",
    "            break\n",
    "            \n",
    "    while next != start:\n",
    "        visited[next]=True\n",
    "        optimal_path.append(next)\n",
    "        #opt_G.add_edge(curr,next,length = G[curr][next]['length'])\n",
    "        # optimal_dist = optimal_dist + G[curr][next]['length']\n",
    "        # finding the shortest Eulerian path from MST\n",
    "        curr = next\n",
    "        for nd in MST.neighbors(curr):\n",
    "            if visited[nd] == False:\n",
    "                next = nd\n",
    "                break\n",
    "        if next == curr:\n",
    "            for nd in G.neighbors(curr):\n",
    "                if visited[nd] == False:\n",
    "                    next = nd\n",
    "                    break\n",
    "        if next == curr:\n",
    "            next = start\n",
    "    \n",
    "    optimal_path.append(next)\n",
    "    \n",
    "    #opt_G.add_edge(curr,next,length = G[curr][next]['length'])\n",
    "    # optimal_dist = optimal_dist + G[curr][next]['length']\n",
    "    # print optimal_dist\n",
    "    \n",
    "    return optimal_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function that adds minimum weight matching edges to MST\n",
    "def minimumWeightedMatching(MST, G, odd_vert):\n",
    "    while odd_vert:\n",
    "        v = odd_vert.pop()\n",
    "        weight = float(\"inf\")\n",
    "        u = 1\n",
    "        closest = 0\n",
    "        for u in odd_vert:\n",
    "            if G[v][u]['weight'] < weight :\n",
    "                weight = G[v][u]['weight']\n",
    "                closest = u\n",
    "        MST.add_edge(v, closest, weight = weight)\n",
    "        odd_vert.remove(closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output path for driver\n",
    "def make_path(list_of_locations, node_paths, tsp_path, starting_index):\n",
    "    output_path = list()\n",
    "    p_prev = starting_index\n",
    "    output_path.append(list_of_locations[starting_index])\n",
    "    \n",
    "    for p_curr in tsp_path[1:]:\n",
    "        output_path.append(list_of_locations[node_paths[(p_prev, p_curr)][1]])\n",
    "        p_prev = p_curr\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for drivers and paths\n",
    "\n",
    "\"\"\"\n",
    "dropped_TAs = Create dictionary of home_indices for whether or not they've been dropped off\n",
    "dropoff_mapping = Create dictionary of every location on output_path and accompanying list for TAs dropped there\n",
    "\n",
    "Iterate through output_path:\n",
    "    If path stop matches home, drop off TA\n",
    "    Mark in dropped_TAs\n",
    "    Add TA to dropoff_mapping for that location (in terms of actual location name - NOT INDEX)\n",
    "    \n",
    "Iterate through tsp_path:\n",
    "    For every node, drop off associated TAs if they haven't been dropped off yet\n",
    "    Mark in dropped_TAs\n",
    "    Add TA to dropoff_mapping for that node (in terms of actual location name - NOT INDEX)\n",
    "    \n",
    "Sort dropoff_mapping keys in order they are reached in output_path\n",
    "return dropoff_mapping\n",
    "\"\"\"\n",
    "\n",
    "def dropoffs(output_path, nodes, tsp_path, home_indices, list_of_locations):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inputs/100_50.in\n",
      "node_path: [5, 8, 17, 6, 30, 29, 5]\n",
      "output path: ['Clearnesse', 'Violetley', 'Oakmere', 'Valhaven', 'Brightmere', 'Bushgate', 'Clearnesse']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-703d65f7f055>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TEST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msolve_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inputs/100_50.in\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"outputs_imam\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-e3a543968056>\u001b[0m in \u001b[0;36msolve_from_file\u001b[1;34m(input_file, output_directory, params)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mnum_of_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_houses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_houses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_car_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjacency_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mcar_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_offs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_locations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_houses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_car_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madjacency_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mbasename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "solve_from_file(\"inputs/100_50.in\", \"outputs_imam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================\n",
    "   No need to change any code below this line\n",
    "======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================================\n",
    "   No need to change any code below this line\n",
    "======================================================================\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Convert solution with path and dropoff_mapping in terms of indices\n",
    "and write solution output in terms of names to path_to_file + file_number + '.out'\n",
    "\n",
    "! ALL in terms of INDICES\n",
    "! dropoff_mapping = dicitonary of {dropoff_loc: [list of TAs dropped off], ...} \n",
    "! path = driver path\n",
    "\"\"\"\n",
    "def convertToFile(path, dropoff_mapping, path_to_file, list_locs):\n",
    "    string = ''\n",
    "    for node in path:\n",
    "        string += list_locs[node] + ' '\n",
    "    string = string.strip()\n",
    "    string += '\\n'\n",
    "\n",
    "    dropoffNumber = len(dropoff_mapping.keys())\n",
    "    string += str(dropoffNumber) + '\\n'\n",
    "    for dropoff in dropoff_mapping.keys():\n",
    "        strDrop = list_locs[dropoff] + ' '\n",
    "        for node in dropoff_mapping[dropoff]:\n",
    "            strDrop += list_locs[node] + ' '\n",
    "        strDrop = strDrop.strip()\n",
    "        strDrop += '\\n'\n",
    "        string += strDrop\n",
    "    utils.write_to_file(path_to_file, string)\n",
    "\n",
    "def solve_from_file(input_file, output_directory, params=[]):\n",
    "    print('Processing', input_file)\n",
    "\n",
    "    input_data = utils.read_file(input_file)\n",
    "    num_of_locations, num_houses, list_locations, list_houses, starting_car_location, adjacency_matrix = data_parser(input_data)\n",
    "    car_path, drop_offs = solve(list_locations, list_houses, starting_car_location, adjacency_matrix, params=params)\n",
    "\n",
    "    basename, filename = os.path.split(input_file)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    output_file = utils.input_to_output(input_file, output_directory)\n",
    "\n",
    "    convertToFile(car_path, drop_offs, output_file, list_locations)\n",
    "\n",
    "\n",
    "def solve_all(input_directory, output_directory, params=[]):\n",
    "    input_files = utils.get_files_with_extension(input_directory, 'in')\n",
    "\n",
    "    for input_file in input_files:\n",
    "        solve_from_file(input_file, output_directory, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--all] input [output_directory] ...\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imamb\\Anaconda3\\envs\\cs170\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Parsing arguments')\n",
    "    parser.add_argument('--all', action='store_true', help='If specified, the solver is run on all files in the input directory. Else, it is run on just the given input file')\n",
    "    parser.add_argument('input', type=str, help='The path to the input file or directory')\n",
    "    parser.add_argument('output_directory', type=str, nargs='?', default='.', help='The path to the directory where the output should be written')\n",
    "    parser.add_argument('params', nargs=argparse.REMAINDER, help='Extra arguments passed in')\n",
    "    args = parser.parse_args()\n",
    "    output_directory = args.output_directory\n",
    "    if args.all:\n",
    "        input_directory = args.input\n",
    "        solve_all(input_directory, output_directory, params=args.params)\n",
    "    else:\n",
    "        input_file = args.input\n",
    "        solve_from_file(input_file, output_directory, params=args.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
