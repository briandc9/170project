{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from student_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import student_utils as s_utils\n",
    "import utils\n",
    "import dwave_networkx as d_nx\n",
    "import dimod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================================================  \n",
    "# Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================================\n",
    "  Complete the following function.\n",
    "======================================================================\n",
    "\"\"\"\n",
    "\n",
    "def solve(list_of_locations, list_of_homes, starting_car_location, adjacency_matrix, params=[]):\n",
    "    \"\"\"\n",
    "    Write your algorithm here.\n",
    "    Input:\n",
    "        list_of_locations: A list of locations such that node i of the graph corresponds to name at index i of the list\n",
    "        list_of_homes: A list of homes\n",
    "        starting_car_location: The name of the starting location for the car\n",
    "        adjacency_matrix: The adjacency matrix from the input file\n",
    "    Output:\n",
    "        A list of locations representing the car path\n",
    "        A dictionary mapping drop-off location to a list of homes of TAs that got off at that particular location\n",
    "        NOTE: both outputs should be in terms of indices not the names of the locations themselves\n",
    "    \"\"\"\n",
    "    # Make networkx graph G\n",
    "    #G = make_graph(adjacency_matrix)\n",
    "    G, message = s_utils.adjacency_matrix_to_graph(adjacency_matrix)\n",
    "    #print(\"G:\", G.edges())\n",
    "    \n",
    "    # Get average distance in between locations\n",
    "    adj = np.array(adjacency_matrix)\n",
    "    adj[adj == \"x\"] = 0\n",
    "    adj = adj.astype(float)\n",
    "    #print(adj[0][0])\n",
    "    \n",
    "    avg_dist = np.mean(adj)\n",
    "    #print(avg_dist)\n",
    "    \n",
    "    # Get indices of homes\n",
    "    home_indices = [list_of_locations.index(i) for i in list_of_homes]\n",
    "    location_indices = range(0, len(list_of_locations))\n",
    "    \n",
    "    starting_index = list_of_locations.index(starting_car_location)\n",
    "    #print(\"start index:\", starting_index)\n",
    "    \n",
    "    # Get adjacency dictionary of distances for each location\n",
    "    adjacencies = make_dictionary(adjacency_matrix, location_indices)\n",
    "    #print(\"adjacencies:\", adjacencies)\n",
    "    \n",
    "    # Get nodes\n",
    "    nodes = make_nodes(adjacencies, location_indices, home_indices, starting_index, avg_dist)\n",
    "    node_roots = list(nodes.keys())\n",
    "    #print(\"node_roots:\", node_roots)\n",
    "    #print(\"nodes:\", nodes)\n",
    "    \n",
    "    # Create graph of just nodes\n",
    "    node_paths, node_G = shortest_paths(G, list_of_locations, node_roots)\n",
    "    #print(\"node_G edges:\", node_G.edges())\n",
    "    #print(\"edge (5,6)\", node_G.get_edge_data(5, 6,default=0) )\n",
    "    #print(\"edge (5,8)\", node_G.get_edge_data(5, 8,default=0) )\n",
    "    #print(node_G.nodes())\n",
    "    #print(\"node_paths:\", node_paths)\n",
    "        \n",
    "    # TSP on node_G\n",
    "    #path = tsp_solver(node_G, starting_index)\n",
    "    tsp_path = christofedes(node_G, starting_index)\n",
    "    #print(\"node_path:\", tsp_path)\n",
    "    \n",
    "    # Output path for driver IN LOCATIONS\n",
    "    output_path = make_path(list_of_locations, node_paths, tsp_path, starting_index)\n",
    "    #print(\"output path:\", output_path)\n",
    "    \n",
    "    #print(\"tsp_path:\", tsp_path, \"\\n output_path:\", output_path, \"\\n homes:\", home_indices, \"\\n nodes:\", nodes)\n",
    "    \n",
    "    # Drop off points and TAs dropped\n",
    "    # dropoff_mapping = dicitonary of {dropoff_loc: [list of TAs dropped off], ...} \n",
    "    dropoff_mapping = dropoffs(output_path, nodes, tsp_path, home_indices, list_of_locations)\n",
    "    \n",
    "    # Create output file\n",
    "    #print(\"output:\", output_path, dropoff_mapping)\n",
    "    return output_path, dropoff_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert adjacency matrix -> pd DataFrame -> networkx Graph\n",
    "def make_graph(adjacency_matrix):\n",
    "    adj = adjacency_matrix\n",
    "    \n",
    "    df = pd.DataFrame(adj)\n",
    "    df = df.replace(\"x\", 0)\n",
    "        \n",
    "    G = nx.from_pandas_adjacency(df)\n",
    "    G.name = 'Graph from pandas adjacency matrix'\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns adjacency dictionary of distances for each location {location: [distances to every location], ...}\n",
    "\n",
    "def make_dictionary(adjacency_matrix, location_indices):\n",
    "    #Create dictionary for every location\n",
    "    adjacencies = {}\n",
    "    \n",
    "    #Create dictionary of adjacencent locations for every location\n",
    "    # adjacencies = {\"loc\" : [distance to every other loc], ...}\n",
    "    # If distance == \"x\" -> None\n",
    "    for i in location_indices:\n",
    "        adj = [None if j == \"x\" else j for j in adjacency_matrix[i]]\n",
    "        adjacencies[i] = adj\n",
    "    \n",
    "    return adjacencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns dictionary of {node_home: [homes belonging to node], ...}\n",
    "\n",
    "#locations = list of indices\n",
    "\n",
    "def make_nodes(adjacencies, locations, home_indices, starting_index, avg_dist):    \n",
    "    #limit = 12000\n",
    "    limit = avg_dist #maximum distance away from node's base (average of all distances in adjacency)\n",
    "    nodes = {} #create a node around every location\n",
    "    \n",
    "    #Create nodes for every home\n",
    "    for loc in locations:\n",
    "        if loc in home_indices:\n",
    "            nodes[loc] = list()\n",
    "            nodes[loc].append(loc) #Start every home's node with itself\n",
    "        else:\n",
    "            nodes[loc] = list()\n",
    "        \n",
    "        for index in home_indices:\n",
    "            distance = adjacencies[loc][index]\n",
    "            if (distance != None) and (distance < limit):\n",
    "                #append other home that is within limit to the node starting at that home\n",
    "                #current = nodes[loc]\n",
    "                nodes[loc].append(index) #returns None    \n",
    "    \n",
    "    #Clean up node dictionary to only contain largest nodes ------------\n",
    "    deleted_nodes = nodes.copy()\n",
    "    #print(\"deleted nodes:\", deleted_nodes)\n",
    "    homes_represented = home_indices\n",
    "    #print(\"homes represented\", homes_represented)\n",
    "    nodes_to_keep = list()\n",
    "    \n",
    "    #for node in node.keys():\n",
    "    \n",
    "    while homes_represented:        \n",
    "        v = list(deleted_nodes.values())\n",
    "        k = list(deleted_nodes.keys())\n",
    "        biggest_node = k[v.index(max(v, key=len))]\n",
    "        \n",
    "        #remove homes that are already included in list\n",
    "        #print(len(homes_represented))\n",
    "        homes_represented = [x for x in homes_represented if x not in nodes[biggest_node]]\n",
    "        \n",
    "        deleted_nodes.pop(biggest_node, None)\n",
    "        \n",
    "        for home in nodes[biggest_node]:\n",
    "            deleted_nodes.pop(home, None)\n",
    "            #print(\"deleted_nodes:\", deleted_nodes)\n",
    "        \n",
    "        nodes_to_keep.append(biggest_node)\n",
    "    \n",
    "    #print(nodes_to_keep)\n",
    "    \n",
    "    if starting_index not in nodes_to_keep:\n",
    "        nodes_to_keep.append(starting_index)\n",
    "    \n",
    "    return {key: nodes[key] for key in nodes_to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary of shortest paths between nodes {(node_1, node_2): [list of path], ...} \n",
    "#    for outputting driver path\n",
    "#returns new graph of just nodes and their associated distances\n",
    "#    for TSP solving with dwave\n",
    "\n",
    "def shortest_paths(G, list_of_locations, node_roots):\n",
    "    node_paths = {}\n",
    "    #node_distances = {}\n",
    "    \n",
    "    #Make new graph of nodes\n",
    "    node_G = nx.Graph()\n",
    "    \n",
    "    for node in node_roots:\n",
    "        node_G.add_node(node)\n",
    "    \n",
    "    #Get shortest path between every node and the distance of that path\n",
    "    for node_s in node_roots:\n",
    "        node_paths[node_s] = list()\n",
    "        for node_t in node_roots:\n",
    "            #index_s = list_of_locations.index(node_s)\n",
    "            #index_t = list_of_locations.index(node_t)\n",
    "            if (node_s != node_t):\n",
    "                path = nx.shortest_path(G, source = node_s, target = node_t, weight = \"weight\")\n",
    "                node_paths[(node_s, node_t)] = path\n",
    "\n",
    "                path_weight = nx.shortest_path_length(G, source= node_s, target= node_t, weight= \"weight\")\n",
    "                node_G.add_edge(node_s, node_t, weight= path_weight)\n",
    "        \n",
    "    return node_paths, node_G\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSP Solver\n",
    "\n",
    "G (NetworkX graph) – The graph on which to find a minimum traveling salesperson route. This should be a complete graph with non-zero weights on every edge.\n",
    "\n",
    "sampler – A binary quadratic model sampler. A sampler is a process that samples from low energy states in models defined by an Ising equation or a Quadratic Unconstrained Binary Optimization Problem (QUBO). A sampler is expected to have a ‘sample_qubo’ and ‘sample_ising’ method. A sampler is expected to return an iterable of samples, in order of increasing energy. If no sampler is provided, one must be provided using the set_default_sampler function.\n",
    "\n",
    "lagrange (number, optional (default 2)) – Lagrange parameter to weight constraints (visit every city once) versus objective (shortest distance route).\n",
    "\n",
    "weight (optional (default 'weight')) – The name of the edge attribute containing the weight.\n",
    "\n",
    "start (node, optional) – If provided, the route will begin at start.\n",
    "\n",
    "sampler_args – Additional keyword parameters are passed to the sampler.\n",
    "\n",
    "### OR...\n",
    "Use Christofedes algo\n",
    "https://medium.com/musoc17-visualization-of-popular-algorithms/travelling-salesman-problem-e3b98653b11a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dwave tsp.python\n",
    "\n",
    "def tsp_solver(node_G, starting_index):\n",
    "    \n",
    "    return d_nx.traveling_salesperson(node_G, sampler = sample_qubo(), weight='weight', start=starting_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def christofedes(G, starting_node):\n",
    "    #print(\"G stuff\", G.nodes())\n",
    "    \n",
    "    #opt_G = nx.DiGraph()\n",
    "    optimal_path = list()\n",
    "    #optimal_dist = 0\n",
    "    \n",
    "    MST = nx.minimum_spanning_tree(G, weight='weight') # generates minimum spanning tree of graph G, using Prim's algo\n",
    "    odd_vert = [] #list containing vertices with odd degree\n",
    "    \n",
    "    for i in MST.nodes():\n",
    "        #print(MST.degree(i))\n",
    "        if MST.degree(i)%2 != 0: \n",
    "            odd_vert.append(i) #if the degree of the vertex is odd, then append it to odd_vert list\n",
    "            \n",
    "    #print(\"odd vertices: \", odd_vert)\n",
    "            \n",
    "    minimumWeightedMatching(MST, G, odd_vert) #adds minimum weight matching edges to MST\n",
    "    \n",
    "    # now MST has the Eulerian circuit\n",
    "    #start = MST.nodes()[0]\n",
    "    start = starting_node\n",
    "    visited = {node: False for node in MST.nodes()}\n",
    "    \n",
    "    # finds the hamiltonian circuit (skips repeated vertices)\n",
    "    curr = start\n",
    "    visited[curr] = True\n",
    "    optimal_path.append(curr)\n",
    "    \n",
    "    #print(MST.neighbors(curr))\n",
    "    next = None\n",
    "    \n",
    "    for nd in MST.neighbors(curr):\n",
    "        #print(nd)\n",
    "        if visited[nd] == False or nd == start:\n",
    "            next = nd\n",
    "            break\n",
    "            \n",
    "    if next == None:\n",
    "        return [start]\n",
    "            \n",
    "    while next != start:\n",
    "        visited[next]=True\n",
    "        optimal_path.append(next)\n",
    "        #opt_G.add_edge(curr,next,length = G[curr][next]['length'])\n",
    "        # optimal_dist = optimal_dist + G[curr][next]['length']\n",
    "        # finding the shortest Eulerian path from MST\n",
    "        curr = next\n",
    "        for nd in MST.neighbors(curr):\n",
    "            if visited[nd] == False:\n",
    "                next = nd\n",
    "                break\n",
    "        if next == curr:\n",
    "            for nd in G.neighbors(curr):\n",
    "                if visited[nd] == False:\n",
    "                    next = nd\n",
    "                    break\n",
    "        if next == curr:\n",
    "            next = start\n",
    "    \n",
    "    optimal_path.append(next)\n",
    "    \n",
    "    #opt_G.add_edge(curr,next,length = G[curr][next]['length'])\n",
    "    # optimal_dist = optimal_dist + G[curr][next]['length']\n",
    "    # print optimal_dist\n",
    "    \n",
    "    return optimal_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function that adds minimum weight matching edges to MST\n",
    "def minimumWeightedMatching(MST, G, odd_vert):\n",
    "    while odd_vert:\n",
    "        v = odd_vert.pop()\n",
    "        weight = float(\"inf\")\n",
    "        u = 1\n",
    "        closest = 0\n",
    "        \n",
    "        #print(\"boo\")\n",
    "        for u in odd_vert:\n",
    "            #print(\"yo\")\n",
    "            if G[v][u]['weight'] < weight :\n",
    "                #print(\"hey\")\n",
    "                weight = G[v][u]['weight']\n",
    "                closest = u\n",
    "        MST.add_edge(v, closest, weight = weight)\n",
    "        odd_vert.remove(closest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output path for driver\n",
    "def make_path(list_of_locations, node_paths, tsp_path, starting_index):\n",
    "    output_path = list()\n",
    "    p_prev = starting_index\n",
    "    output_path.append(starting_index)\n",
    "    \n",
    "    #print(\"node paths:\", node_paths)\n",
    "    \n",
    "    for p_curr in tsp_path[1:]:\n",
    "        if node_paths[(p_prev, p_curr)][1]:\n",
    "            path = node_paths[(p_prev, p_curr)][1:]\n",
    "            output_path.extend(path)\n",
    "        else:\n",
    "            path = node_paths[(p_curr, p_prev)][::-1] \n",
    "            path = path[1:]\n",
    "            output_path.extend(path)\n",
    "            \n",
    "        p_prev = p_curr\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for drivers and paths\n",
    "\n",
    "\"\"\"\n",
    "dropped_TAs = Create dictionary of home_indices for whether or not they've been dropped off\n",
    "dropoff_mapping = Create dictionary of every location on output_path and accompanying list for TAs dropped there\n",
    "\n",
    "Iterate through output_path:\n",
    "    If path stop matches home, drop off TA\n",
    "    Mark in dropped_TAs\n",
    "    Add TA to dropoff_mapping for that location (in terms of INDEX)\n",
    "    \n",
    "Iterate through tsp_path:\n",
    "    For every node, drop off associated TAs if they haven't been dropped off yet\n",
    "    Mark in dropped_TAs\n",
    "    Add TA to dropoff_mapping for that node (in terms of INDEX)\n",
    "    \n",
    "Sort dropoff_mapping keys in order they are reached in output_path\n",
    "return dropoff_mapping\n",
    "\"\"\"\n",
    "\n",
    "def dropoffs(output_path, nodes, tsp_path, home_indices, list_of_locations):\n",
    "    dropped_TAs = {}\n",
    "    dropoff_mapping = {}\n",
    "    \n",
    "    for home in home_indices:\n",
    "        dropped_TAs[home] = False\n",
    "    \n",
    "    for loc in output_path:\n",
    "        if loc in home_indices:\n",
    "            if not dropped_TAs[loc]:\n",
    "                dropped_TAs[loc] = True\n",
    "                \n",
    "                if not loc in dropoff_mapping.keys():\n",
    "                    dropoff_mapping[loc] = list()\n",
    "                dropoff_mapping[loc].append(loc)\n",
    "        \n",
    "    for loc in output_path:\n",
    "        if loc in tsp_path:\n",
    "            for ta in nodes[loc]:\n",
    "                if not dropped_TAs[ta]:\n",
    "                    dropped_TAs[ta] = True\n",
    "                    \n",
    "                    if not loc in dropoff_mapping.keys():\n",
    "                        dropoff_mapping[loc] = list()\n",
    "                    dropoff_mapping[loc].append(ta)\n",
    "    \n",
    "    for ta in list(dropped_TAs.keys()):\n",
    "        if not dropped_TAs[ta]:\n",
    "            raise ValueError(\"This TA hasn't been dropped off: \", ta)\n",
    "    \n",
    "    #sort dropoffs?\n",
    "    return dropoff_mapping\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inputs/97_100.in\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "solve_from_file(\"inputs/97_100.in\", \"test_output_imam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================\n",
    "   No need to change any code below this line\n",
    "======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================================\n",
    "   No need to change any code below this line\n",
    "======================================================================\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Convert solution with path and dropoff_mapping in terms of indices\n",
    "and write solution output in terms of names to path_to_file + file_number + '.out'\n",
    "\n",
    "! ALL in terms of INDICES\n",
    "! dropoff_mapping = dicitonary of {dropoff_loc: [list of TAs dropped off], ...} \n",
    "! path = driver path\n",
    "\"\"\"\n",
    "def convertToFile(path, dropoff_mapping, path_to_file, list_locs):\n",
    "    string = ''\n",
    "    for node in path:\n",
    "        string += list_locs[node] + ' '\n",
    "    string = string.strip()\n",
    "    string += '\\n'\n",
    "\n",
    "    dropoffNumber = len(dropoff_mapping.keys())\n",
    "    string += str(dropoffNumber) + '\\n'\n",
    "    for dropoff in dropoff_mapping.keys():\n",
    "        strDrop = list_locs[dropoff] + ' '\n",
    "        for node in dropoff_mapping[dropoff]:\n",
    "            strDrop += list_locs[node] + ' '\n",
    "        strDrop = strDrop.strip()\n",
    "        strDrop += '\\n'\n",
    "        string += strDrop\n",
    "    utils.write_to_file(path_to_file, string)\n",
    "\n",
    "def solve_from_file(input_file, output_directory, params=[]):\n",
    "    print('Processing', input_file)\n",
    "\n",
    "    input_data = utils.read_file(input_file)\n",
    "    num_of_locations, num_houses, list_locations, list_houses, starting_car_location, adjacency_matrix = data_parser(input_data)\n",
    "    car_path, drop_offs = solve(list_locations, list_houses, starting_car_location, adjacency_matrix, params=params)\n",
    "\n",
    "    basename, filename = os.path.split(input_file)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    output_file = utils.input_to_output(input_file, output_directory)\n",
    "\n",
    "    convertToFile(car_path, drop_offs, output_file, list_locations)\n",
    "\n",
    "\n",
    "def solve_all(input_directory, output_directory, params=[]):\n",
    "    input_files = utils.get_files_with_extension(input_directory, 'in')\n",
    "\n",
    "    for input_file in input_files:\n",
    "        solve_from_file(input_file, output_directory, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--all] input [output_directory] ...\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imamb\\Anaconda3\\envs\\cs170\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Parsing arguments')\n",
    "    parser.add_argument('--all', action='store_true', help='If specified, the solver is run on all files in the input directory. Else, it is run on just the given input file')\n",
    "    parser.add_argument('input', type=str, help='The path to the input file or directory')\n",
    "    parser.add_argument('output_directory', type=str, nargs='?', default='.', help='The path to the directory where the output should be written')\n",
    "    parser.add_argument('params', nargs=argparse.REMAINDER, help='Extra arguments passed in')\n",
    "    args = parser.parse_args()\n",
    "    output_directory = args.output_directory\n",
    "    if args.all:\n",
    "        input_directory = args.input\n",
    "        solve_all(input_directory, output_directory, params=args.params)\n",
    "    else:\n",
    "        input_file = args.input\n",
    "        solve_from_file(input_file, output_directory, params=args.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
